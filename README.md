# AIMEE Backend API v2.0

AIé…ç½®æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ  - ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¬ãƒ‡ã‚£ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

## ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦

AIMEEï¼ˆAIé…ç½®æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼‰ã¯ã€**è»½é‡LLM** + **å°‚é–€è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³** + **ãƒ¡ã‚¤ãƒ³LLM** ã®ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€åŠ´åƒåŠ›é…ç½®ã®æœ€é©åŒ–ã‚’1-8ç§’ã§å®Ÿç¾ã—ã¾ã™ã€‚

### æ ¸å¿ƒè¨­è¨ˆæ€æƒ³

```mermaid
graph TB
    Query[ğŸ‘¤ è‡ªç„¶è¨€èªè¦æ±‚] --> LightLLM[ğŸ¤– è»½é‡LLM<br/>qwen2:0.5b<br/>æ„å›³è§£æ 0.2ç§’]
    
    LightLLM --> Parallel[ğŸ“Š ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†]
    
    subgraph "ä¸¦åˆ—å°‚é–€ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆLLMãªã— 1-3ç§’ï¼‰"
        Parallel --> MySQL[ğŸ“Š æ•°å€¤åˆ†æã‚¨ãƒ³ã‚¸ãƒ³<br/>SQLé›†è¨ˆãƒ»çµ±è¨ˆè¨ˆç®—]
        Parallel --> Rules[ğŸ§© åˆ¶ç´„å……è¶³ã‚¨ãƒ³ã‚¸ãƒ³<br/>ç·šå½¢è¨ˆç”»æ³•ãƒ»æœ€é©åŒ–]
        Parallel --> Vector[ğŸ” ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³<br/>ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢]
    end
    
    MySQL --> MainLLM[ğŸ¤– ãƒ¡ã‚¤ãƒ³LLM<br/>gemma3:4b-instruct<br/>çµ±åˆåˆ¤æ–­ 3-5ç§’]
    Rules --> MainLLM
    Vector --> MainLLM
    
    MainLLM --> Response[ğŸ¯ æœ€çµ‚å›ç­”]
    
    subgraph "Dockeræœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿åŸºç›¤"
        MySQL --> AimeeDB[(ğŸ—ƒï¸ aimee_db<br/>14ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿)]
        Vector --> ChromaDB[(ğŸ—„ï¸ ChromaDB<br/>ç®¡ç†è€…ãƒã‚¦ãƒã‚¦)]
        MainLLM --> Redis[(âš¡ Redis<br/>é«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥)]
    end
    
    style LightLLM fill:#e1f5fe
    style MainLLM fill:#f9d71c
    style MySQL fill:#ffb74d
    style Rules fill:#64b5f6
    style Vector fill:#9575cd
```

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç‰¹å¾´

### 1. ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–æˆ¦ç•¥

**éšå±¤åŒ–LLMå®Ÿè¡Œ**ï¼š
- ğŸš€ **è»½é‡LLM**: æ„å›³è§£æï¼ˆqwen2:0.5bã€2GBã€0.2ç§’ï¼‰
- ğŸ§  **ãƒ¡ã‚¤ãƒ³LLM**: å…¨ã¦ã®å›ç­”ç”Ÿæˆï¼ˆgemma3:4b-instructã€12GBã€1-5ç§’ï¼‰

**å°‚é–€ã‚¨ãƒ³ã‚¸ãƒ³ä¸¦åˆ—å®Ÿè¡Œ**ï¼š
- ğŸ“Š **æ•°å€¤åˆ†æ**: MySQLé›†è¨ˆã«ã‚ˆã‚‹ç¢ºå®Ÿãªçµ±è¨ˆè¨ˆç®—
- ğŸ§© **åˆ¶ç´„å……è¶³**: æ•°å­¦çš„æœ€é©åŒ–ã«ã‚ˆã‚‹å®Ÿè¡Œå¯èƒ½è§£æ¢ç´¢
- ğŸ” **ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢**: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹æ„å‘³çš„é–¢é€£æŠ½å‡º

### 2. å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é€£æº

```mermaid
graph LR
    subgraph "aimee_dbï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼‰"
        EMP[employees<br/>å¾“æ¥­å“¡ãƒã‚¹ã‚¿]
        PROD[productivity_records<br/>ç”Ÿç”£æ€§å®Ÿç¸¾]
        ATTEND[attendance_records<br/>å‹¤æ€ å®Ÿç¸¾]
        ASSIGN[daily_assignments<br/>é…ç½®è¨ˆç”»]
        ADJUST[assignment_adjustments<br/>èª¿æ•´å±¥æ­´]
        FORECAST[workload_forecasts<br/>æ¥­å‹™é‡äºˆæ¸¬]
    end
    
    subgraph "ChromaDBï¼ˆéæ§‹é€ åŒ–ãƒŠãƒ¬ãƒƒã‚¸ï¼‰"
        RULES[é…ç½®ãƒ«ãƒ¼ãƒ«ãƒ»åˆ¶ç´„æ¡ä»¶]
        KNOWHOW[ç¾å ´ãƒã‚¦ãƒã‚¦ãƒ»çµŒé¨“å‰‡]
        CASES[æˆåŠŸäº‹ä¾‹ãƒ»å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³]
        CONTEXT[rag_contextè“„ç©ãƒ‡ãƒ¼ã‚¿]
    end
    
    subgraph "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†"
        MySQL[æ•°å€¤è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³] --> Gemma3[çµ±åˆåˆ¤æ–­AI]
        ChromaDB[ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢] --> Gemma3
        Gemma3 --> Redis[çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥]
    end
    
    style MySQL fill:#4fc3f7
    style ChromaDB fill:#81c784
    style Gemma3 fill:#f9d71c
```

### 3. Dockeré«˜é€ŸåŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
graph TB
    subgraph "Docker Compose Production Environment"
        subgraph "Webå±¤ï¼ˆNginxæœ€é©åŒ–ï¼‰"
            Nginx[ğŸŒ Nginx<br/>:80<br/>ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥]
            Frontend[âš›ï¸ Management UI<br/>:3000<br/>Reactç®¡ç†ç”»é¢]
        end
        
        subgraph "APIå±¤ï¼ˆFastAPIæœ€é©åŒ–ï¼‰"
            Backend[ğŸ FastAPI Backend<br/>:8000<br/>éåŒæœŸå‡¦ç†ãƒ»ä¸¦åˆ—å®Ÿè¡Œ]
        end
        
        subgraph "AIåŸºç›¤å±¤ï¼ˆGPUæœ€é©åŒ–ï¼‰"
            OllamaLight[ğŸ¤– Ollama Lightweight<br/>:11433<br/>qwen2:0.5b]
            OllamaMain[ğŸ¤– Ollama Main<br/>:11434<br/>gemma3:4b-instruct]
            ChromaDB[ğŸ—„ï¸ ChromaDB<br/>:8001<br/>æ°¸ç¶šãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢]
        end
        
        subgraph "ãƒ‡ãƒ¼ã‚¿å±¤ï¼ˆSSDæœ€é©åŒ–ï¼‰"
            MySQL[ğŸ—ƒï¸ MySQL 8.0<br/>:3306<br/>aimee_db 14ãƒ†ãƒ¼ãƒ–ãƒ«]
            Redis[âš¡ Redis Cluster<br/>:6379<br/>L1/L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥]
        end
        
        subgraph "æ°¸ç¶šåŒ–ãƒœãƒªãƒ¥ãƒ¼ãƒ ï¼ˆNVMe SSDï¼‰"
            MySQLVol[ğŸ’¾ mysql-data]
            ChromaVol[ğŸ“Š chroma-vectors] 
            OllamaVol[ğŸ¤– ollama-models]
            RedisVol[âš¡ redis-snapshots]
        end
    end
    
    Nginx --> Frontend
    Nginx --> Backend
    Backend --> OllamaLight
    Backend --> OllamaMain
    Backend --> ChromaDB
    Backend --> MySQL
    Backend --> Redis
    
    MySQL --- MySQLVol
    ChromaDB --- ChromaVol
    OllamaLight --- OllamaVol
    OllamaMain --- OllamaVol
    Redis --- RedisVol
    
    style Backend fill:#f9d71c
    style OllamaLight fill:#e1f5fe
    style OllamaMain fill:#ff9800
    style MySQL fill:#4fc3f7
    style ChromaDB fill:#64b5f6
```

## APIä»•æ§˜

### çµ±åˆAIç›¸è«‡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

#### ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½
- `POST /api/v1/ai/optimize` - é…ç½®æœ€é©åŒ–ç›¸è«‡ï¼ˆè¤‡é›‘åˆ†æï¼š6-8ç§’ï¼‰
- `POST /api/v1/ai/query` - ç°¡å˜ãªè³ªå•ï¼ˆå³ç­”ï¼š0.5ç§’ï¼‰
- `WebSocket /ws/ai/stream` - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç›¸è«‡
- `POST /api/v1/ai/analyze` - ç”Ÿç”£æ€§åˆ†æç›¸è«‡

#### RAGç®¡ç†
- `POST /api/v1/knowledge/documents` - ç®¡ç†è€…ãƒã‚¦ãƒã‚¦ç™»éŒ²
- `GET /api/v1/knowledge/search` - ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢
- `DELETE /api/v1/knowledge/documents/{doc_id}` - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤
- `POST /api/v1/knowledge/context` - RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè“„ç©

#### ãƒ‡ãƒ¼ã‚¿åˆ†æ
- `GET /api/v1/analytics/productivity` - ç”Ÿç”£æ€§ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆMySQLé›†è¨ˆï¼‰
- `GET /api/v1/analytics/attendance` - å‹¤æ€ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
- `GET /api/v1/analytics/alerts` - ã‚¢ãƒ©ãƒ¼ãƒˆçµ±è¨ˆ
- `POST /api/v1/analytics/forecast` - æ¥­å‹™é‡äºˆæ¸¬

#### å¾“æ¥æ©Ÿèƒ½ï¼ˆAIå¼·åŒ–ç‰ˆï¼‰

##### ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ï¼ˆAIåˆ†æå¼·åŒ–ï¼‰
- `GET /api/v1/alerts` - ã‚¢ãƒ©ãƒ¼ãƒˆä¸€è¦§å–å¾—
- `GET /api/v1/alerts/{alert_id}` - ã‚¢ãƒ©ãƒ¼ãƒˆè©³ç´°å–å¾—
- `POST /api/v1/alerts/{alert_id}/analyze` - AIåŸå› åˆ†æ
- `POST /api/v1/alerts/{alert_id}/improve` - AIæ”¹å–„ææ¡ˆ

##### é…ç½®ç®¡ç†ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–ï¼‰
- `GET /api/v1/assignments/current` - ç¾åœ¨ã®é…ç½®å–å¾—
- `POST /api/v1/assignments/recommend` - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–ææ¡ˆ
- `PUT /api/v1/assignments/{assignment_id}` - é…ç½®æ›´æ–°
- `POST /api/v1/assignments/simulate` - é…ç½®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

##### ãƒãƒ£ãƒƒãƒˆï¼ˆãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰
- `POST /api/v1/chat/messages` - è‡ªç„¶è¨€èªæ¥­å‹™ç›¸è«‡
- `GET /api/v1/chat/history` - ç›¸è«‡å±¥æ­´å–å¾—
- `WebSocket /ws/chat/{session_id}` - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±

## å®Ÿè£…ä¾‹ï¼šå®Œå…¨ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼

### è¤‡é›‘ãªè¦æ±‚ï¼šã€Œæ¨ªæµœæ‹ ç‚¹ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’æ”¹å–„ã—ã¦ã€

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¾‹
```bash
curl -X POST http://localhost:8000/api/v1/ai/optimize \
  -H "Content-Type: application/json" \
  -d '{
    "query": "æ¨ªæµœæ‹ ç‚¹ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’æ”¹å–„ã—ã¦",
    "context": {
      "user_id": "manager001",
      "session_id": "sess_20241121_001",
      "urgency": "medium"
    }
  }'
```

#### å‡¦ç†ãƒ•ãƒ­ãƒ¼è©³ç´°
```python
# ã‚¹ãƒ†ãƒƒãƒ—1: è»½é‡LLMæ„å›³è§£æï¼ˆ0.2ç§’ï¼‰
async def parse_intent_lightweight(query):
    prompt = f"""ä»¥ä¸‹ã‚’è§£æã—ã¦JSONå½¢å¼ã§å›ç­”:
å…¥åŠ›: "{query}"
{{
    "intent": "improve_alerts|optimize_assignment|simple_query",
    "location": "æ¨ªæµœ|å“å·|æœ­å¹Œ|å¤§é˜ª|ä½ä¸–ä¿|unclear",
    "complexity": "simple|complex",
    "confidence": 0.0-1.0
}}"""
    
    response = await qwen2_0_5b.generate(prompt, max_tokens=80)
    return json.loads(response)

# ã‚¹ãƒ†ãƒƒãƒ—2: ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆ1-3ç§’ã€LLMãªã—ï¼‰
async def collect_data_parallel(intent):
    mysql_task = analyze_mysql_data(intent["location"])
    constraint_task = analyze_constraints(intent["location"])
    knowledge_task = search_knowledge_base(f"{intent['location']} ã‚¢ãƒ©ãƒ¼ãƒˆ æ”¹å–„")
    
    return await asyncio.gather(mysql_task, constraint_task, knowledge_task)

# MySQLæ•°å€¤åˆ†æï¼ˆ1ç§’ï¼‰
async def analyze_mysql_data(location):
    queries = {
        "current_alerts": """
            SELECT 
                adjustment_type,
                COUNT(*) as count,
                AVG(ai_confidence_score) as avg_confidence
            FROM assignment_adjustments aa
            JOIN daily_assignments da ON aa.original_assignment_id = da.assignment_id
            WHERE da.location_id = (SELECT location_id FROM locations WHERE location_name = %s)
            AND aa.adjustment_date >= CURDATE() - INTERVAL 7 DAY
            GROUP BY adjustment_type
        """,
        
        "productivity_trend": """
            SELECT 
                record_date,
                AVG(productivity_score) as daily_productivity,
                COUNT(*) as record_count
            FROM productivity_records pr
            JOIN employees e ON pr.employee_id = e.employee_id
            WHERE e.location_id = (SELECT location_id FROM locations WHERE location_name = %s)
            AND pr.record_date >= CURDATE() - INTERVAL 30 DAY
            GROUP BY record_date
            ORDER BY record_date DESC
            LIMIT 7
        """,
        
        "attendance_pattern": """
            SELECT 
                DAYOFWEEK(attendance_date) as day_of_week,
                attendance_status,
                COUNT(*) as count
            FROM attendance_records ar
            JOIN employees e ON ar.employee_id = e.employee_id
            WHERE e.location_id = (SELECT location_id FROM locations WHERE location_name = %s)
            AND ar.attendance_date >= CURDATE() - INTERVAL 30 DAY
            GROUP BY DAYOFWEEK(attendance_date), attendance_status
        """
    }
    
    results = {}
    for key, query in queries.items():
        results[key] = await execute_query(query, (location,))
    
    return results

# åˆ¶ç´„å……è¶³åˆ†æï¼ˆ2ç§’ï¼‰
async def analyze_constraints(location):
    from scipy.optimize import linprog
    
    # åˆ©ç”¨å¯èƒ½äººå“¡ã®å–å¾—
    staff_query = """
        SELECT 
            e.employee_id,
            e.skill_level,
            AVG(pr.productivity_score) as avg_productivity,
            COUNT(CASE WHEN ar.attendance_status = 'present' THEN 1 END) * 1.0 / COUNT(*) as attendance_rate
        FROM employees e
        LEFT JOIN productivity_records pr ON e.employee_id = pr.employee_id 
            AND pr.record_date >= CURDATE() - INTERVAL 30 DAY
        LEFT JOIN attendance_records ar ON e.employee_id = ar.employee_id 
            AND ar.attendance_date >= CURDATE() - INTERVAL 30 DAY
        WHERE e.location_id = (SELECT location_id FROM locations WHERE location_name = %s)
        AND e.is_active = 1
        GROUP BY e.employee_id, e.skill_level
    """
    
    staff_data = await execute_query(staff_query, (location,))
    
    # ç·šå½¢è¨ˆç”»æ³•ã«ã‚ˆã‚‹æœ€é©åŒ–
    c = [-staff["avg_productivity"] for staff in staff_data]
    A_eq, b_eq = build_constraint_matrix(staff_data)
    
    result = linprog(c, A_eq=A_eq, b_eq=b_eq, method='highs')
    
    return {
        "available_staff": len(staff_data),
        "optimization_feasible": result.success,
        "optimal_productivity": -result.fun if result.success else 0,
        "constraint_violations": [] if result.success else ["äººå“¡ä¸è¶³"]
    }

# ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ï¼ˆ0.5ç§’ï¼‰
async def search_knowledge_base(query):
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
    query_vector = embedding_model.encode(query)
    vector_results = chromadb_collection.query(
        query_embeddings=[query_vector.tolist()],
        n_results=5
    )
    
    # DBè“„ç©ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢
    context_query = """
        SELECT context_value, relevance_score
        FROM rag_context 
        WHERE context_key LIKE %s
        ORDER BY relevance_score DESC, use_count DESC
        LIMIT 3
    """
    
    db_results = await execute_query(context_query, (f"%{query}%",))
    
    return {
        "vector_knowledge": vector_results['documents'][0],
        "db_knowledge": [item["context_value"] for item in db_results]
    }

# ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¡ã‚¤ãƒ³LLMçµ±åˆåˆ¤æ–­ï¼ˆ3-5ç§’ï¼‰
async def generate_integrated_solution(intent, mysql_data, constraint_data, knowledge_data):
    integrated_prompt = f"""åŠ´åƒåŠ›ç®¡ç†ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¨ã—ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’çµ±åˆåˆ†æã—ã€å…·ä½“çš„ã§å®Ÿè¡Œå¯èƒ½ãªæ”¹å–„ãƒ—ãƒ©ãƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

ã€è¦æ±‚ã€‘
{intent['intent']}ï¼š{intent['location']}æ‹ ç‚¹ã®ã‚¢ãƒ©ãƒ¼ãƒˆæ”¹å–„

ã€ç¾çŠ¶åˆ†æï¼ˆMySQLé›†è¨ˆçµæœï¼‰ã€‘
ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”ŸçŠ¶æ³:
{format_mysql_alerts(mysql_data['current_alerts'])}

ç”Ÿç”£æ€§ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆç›´è¿‘7æ—¥ï¼‰:
{format_productivity_trend(mysql_data['productivity_trend'])}

å‹¤æ€ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:
{format_attendance_pattern(mysql_data['attendance_pattern'])}

ã€åˆ¶ç´„æ¡ä»¶ï¼ˆæœ€é©åŒ–è¨ˆç®—çµæœï¼‰ã€‘
- åˆ©ç”¨å¯èƒ½äººå“¡: {constraint_data['available_staff']}å
- æœ€é©åŒ–å®Ÿè¡Œå¯èƒ½: {'å¯èƒ½' if constraint_data['optimization_feasible'] else 'åˆ¶ç´„ã‚ã‚Š'}
- äºˆæƒ³æœ€é©ç”Ÿç”£æ€§: {constraint_data['optimal_productivity']:.1f}%
- åˆ¶ç´„é•å: {', '.join(constraint_data['constraint_violations']) if constraint_data['constraint_violations'] else 'ãªã—'}

ã€é–¢é€£ãƒã‚¦ãƒã‚¦ï¼ˆRAGæ¤œç´¢çµæœï¼‰ã€‘
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒã‚¦ãƒã‚¦:
{chr(10).join([f"- {k}" for k in knowledge_data['vector_knowledge']])}

è“„ç©ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{chr(10).join([f"- {k}" for k in knowledge_data['db_knowledge']])}

ã€å›ç­”è¦æ±‚ã€‘
1. å•é¡Œã®å„ªå…ˆé †ä½ä»˜ã‘ï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿æ ¹æ‹ ï¼‰
2. å…·ä½“çš„æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ï¼ˆåˆ¶ç´„æ¡ä»¶è€ƒæ…®ï¼‰
3. æœŸå¾…åŠ¹æœã®å®šé‡çš„äºˆæ¸¬
4. å®Ÿæ–½ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆå®Ÿè¡Œå¯èƒ½æ€§é‡è¦–ï¼‰
5. æˆåŠŸæŒ‡æ¨™ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ–¹æ³•

å¿…ãšåˆ¶ç´„æ¡ä»¶ã‚’éµå®ˆã—ã€éå»ã®ãƒã‚¦ãƒã‚¦ã‚’æ´»ç”¨ã—ã¦å®Ÿç”¨çš„ãªãƒ—ãƒ©ãƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"""

    response = await gemma3_4b.generate(
        integrated_prompt, 
        max_tokens=800, 
        temperature=0.7,
        stream=True
    )
    
    return response
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹
```json
{
  "status": "success",
  "processing_time": "6.8ç§’",
  "model_chain": ["qwen2:0.5b", "gemma3:4b-instruct"],
  "request": "æ¨ªæµœæ‹ ç‚¹ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’æ”¹å–„ã—ã¦",
  
  "improvement_plan": {
    "priority_analysis": {
      "1": "æ®‹æ¥­æ™‚é–“è¶…éã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆä»¶æ•°3ã€é‡è¦åº¦4.2ï¼‰",
      "2": "ç”Ÿç”£æ€§ä½ä¸‹ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆä»¶æ•°5ã€ãƒˆãƒ¬ãƒ³ãƒ‰æ‚ªåŒ–ï¼‰", 
      "3": "æ¬ å‹¤ç‡ä¸Šæ˜‡ï¼ˆæœˆæ›œæ—¥é›†ä¸­ã€15%ä¸Šæ˜‡ï¼‰"
    },
    
    "action_plan": [
      {
        "action": "æœˆæ›œæœã®é…ç½®èª¿æ•´",
        "method": "ç†Ÿç·´è€…2åã‚’åˆå‰é…ç½®ã€æ–°äººã¯åˆå¾Œã‹ã‚‰",
        "expected_effect": "æ¬ å‹¤ç‡5%å‰Šæ¸›ã€ç”Ÿç”£æ€§10%å‘ä¸Š",
        "timeline": "å³æ—¥å®Ÿæ–½å¯èƒ½",
        "cost": "0å††"
      },
      {
        "action": "æ®‹æ¥­å‰Šæ¸›ã®æ¥­å‹™é…åˆ†è¦‹ç›´ã—",
        "method": "16æ™‚ä»¥é™ã®æ–°è¦æ¥­å‹™åœæ­¢ã€ç¿Œæ—¥åˆå‰ã«å›ã™",
        "expected_effect": "æ®‹æ¥­æ™‚é–“30%å‰Šæ¸›",
        "timeline": "3æ—¥ä»¥å†…",
        "cost": "0å††"
      }
    ],
    
    "quantitative_forecast": {
      "ç”Ÿç”£æ€§å‘ä¸Š": "ç¾åœ¨85% â†’ äºˆæ¸¬92%ï¼ˆ7%å‘ä¸Šï¼‰",
      "ã‚¢ãƒ©ãƒ¼ãƒˆå‰Šæ¸›": "é€±5ä»¶ â†’ äºˆæ¸¬2ä»¶ï¼ˆ60%å‰Šæ¸›ï¼‰",
      "ROI": "æœˆé–“200ä¸‡å††ã®ç”Ÿç”£æ€§å‘ä¸ŠåŠ¹æœ"
    },
    
    "implementation_schedule": {
      "Day1": "æœˆæ›œé…ç½®èª¿æ•´é–‹å§‹",
      "Day3": "æ®‹æ¥­å‰Šæ¸›ãƒ«ãƒ¼ãƒ«é©ç”¨",
      "Day7": "åŠ¹æœæ¸¬å®šãƒ»èª¿æ•´",
      "Day14": "å®Œå…¨å®šç€ç¢ºèª"
    }
  },
  
  "supporting_data": {
    "mysql_analysis": {
      "alert_count": 8,
      "productivity_avg": 85.2,
      "attendance_rate": 91.5
    },
    "constraint_check": {
      "available_staff": 15,
      "optimization_feasible": true,
      "constraint_violations": []
    },
    "knowledge_applied": [
      "æ¨ªæµœæ‹ ç‚¹ã§ã¯æœˆæ›œæœã®ç”Ÿç”£æ€§ãŒ15%ä½ä¸‹ã™ã‚‹å‚¾å‘",
      "æ®‹æ¥­æ™‚é–“ã¯16æ™‚ä»¥é™ã®æ–°è¦æ¥­å‹™åœæ­¢ã§30%å‰Šæ¸›å®Ÿç¸¾"
    ]
  },
  
  "next_monitoring": {
    "daily_check": ["æ®‹æ¥­æ™‚é–“", "ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿä»¶æ•°"],
    "weekly_review": ["ç”Ÿç”£æ€§ç‡", "æ¬ å‹¤ç‡", "é…ç½®åŠ¹æœ"],
    "success_criteria": "ã‚¢ãƒ©ãƒ¼ãƒˆä»¶æ•°50%å‰Šæ¸›ã€ç”Ÿç”£æ€§90%é”æˆ"
  }
}
```

### ç°¡å˜ãªè³ªå•ï¼šã€Œç¾åœ¨ã®ã‚¢ãƒ©ãƒ¼ãƒˆä»¶æ•°ã¯ï¼Ÿã€

#### å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ¡ã‚¤ãƒ³LLMã§ç¢ºå®Ÿå›ç­”ï¼š1-2ç§’ï¼‰
```python
# ã‚¹ãƒ†ãƒƒãƒ—1: æ„å›³è§£æï¼ˆ0.2ç§’ï¼‰
intent = await qwen2_0_5b.parse("ç¾åœ¨ã®ã‚¢ãƒ©ãƒ¼ãƒˆä»¶æ•°ã¯ï¼Ÿ")
# â†’ {"intent": "simple_query", "data_required": "alerts"}

# ã‚¹ãƒ†ãƒƒãƒ—2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ0.5ç§’ï¼‰
mysql_result = await execute_query(
    "SELECT COUNT(*) as count FROM assignment_adjustments WHERE adjustment_date = CURDATE()"
)

# ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¡ã‚¤ãƒ³LLMã§æ­£ç¢ºå›ç­”ï¼ˆ1ç§’ï¼‰
response = await gemma3_4b.generate(f"""
ä»¥ä¸‹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«è‡ªç„¶ãªæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„:
è³ªå•: ç¾åœ¨ã®ã‚¢ãƒ©ãƒ¼ãƒˆä»¶æ•°ã¯ï¼Ÿ
ãƒ‡ãƒ¼ã‚¿: {mysql_result[0]['count']}ä»¶ã®ã‚¢ãƒ©ãƒ¼ãƒˆãŒä»Šæ—¥ç™ºç”Ÿ

æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
""")

return {
    "answer": response,
    "processing_time": "1.7ç§’",
    "model_used": "gemma3:4b-instruct",
    "data_source": "real_time_mysql"
}
```

## Dockeræœ€é©åŒ–ç’°å¢ƒ

### ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆè¶…é«˜é€Ÿï¼‰
```bash
# ç’°å¢ƒæ§‹ç¯‰ï¼ˆ5åˆ†ï¼‰
git clone <repository-url>
cd aimee-be

# è¨­å®šï¼ˆDockeræœ€é©åŒ–æ¸ˆã¿ï¼‰
cp .env.example .env

# å…¨ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•ï¼ˆGPUåŠ é€Ÿï¼‰
make setup-production
make download-models-parallel
make dev-optimized

# ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª
curl http://localhost:8000/health
```

### Docker Composeè¨­å®šï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
```yaml
version: '3.9'

services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/cache:/var/cache/nginx
    depends_on:
      - backend
      - frontend
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.production
    environment:
      - DATABASE_URL=mysql://aimee_user:aimee_pass@mysql:3306/aimee_db
      - REDIS_URL=redis://redis:6379/0
      - OLLAMA_LIGHT_HOST=ollama-light
      - OLLAMA_MAIN_HOST=ollama-main
      - CHROMADB_HOST=chromadb
    volumes:
      - ./backend:/app
      - /app/__pycache__
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama-light:
        condition: service_healthy
      ollama-main:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
    restart: unless-stopped

  # è»½é‡LLMå°‚ç”¨ã‚³ãƒ³ãƒ†ãƒŠ
  ollama-light:
    image: ollama/ollama:latest
    ports:
      - "11433:11434"
    volumes:
      - ollama-light-models:/root/.ollama
    environment:
      - OLLAMA_MODELS=qwen2:0.5b
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_LOAD_TIMEOUT=600
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ãƒ¡ã‚¤ãƒ³LLMå°‚ç”¨ã‚³ãƒ³ãƒ†ãƒŠ  
  ollama-main:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-main-models:/root/.ollama
    environment:
      - OLLAMA_MODELS=gemma3:4b-instruct
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_GPU_LAYERS=20
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  mysql:
    image: mysql:8.0
    ports:
      - "3306:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=root_password
      - MYSQL_DATABASE=aimee_db
      - MYSQL_USER=aimee_user
      - MYSQL_PASSWORD=aimee_pass
    volumes:
      - mysql-data:/var/lib/mysql
      - ./aimee-db/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql
      - ./aimee-db/sample_data.sql:/docker-entrypoint-initdb.d/02-sample_data.sql
    command: >
      --innodb-buffer-pool-size=2G
      --innodb-log-file-size=512M
      --max-connections=200
      --query-cache-size=64M
    deploy:
      resources:
        limits:
          memory: 4G
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma-data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma/chroma
      - CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=token
      - CHROMA_SERVER_AUTH_CREDENTIALS=aimee-chroma-token
      - CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER=X-Chroma-Token
    deploy:
      resources:
        limits:
          memory: 4G
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: >
      redis-server 
      --appendonly yes 
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 60
    deploy:
      resources:
        limits:
          memory: 2G
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

volumes:
  mysql-data:
    driver: local
  chroma-data:
    driver: local
  redis-data:
    driver: local
  ollama-light-models:
    driver: local
  ollama-main-models:
    driver: local

networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

### ç’°å¢ƒè¨­å®šï¼ˆãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–ï¼‰

```bash
# AI/LLMè¨­å®šï¼ˆãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ï¼‰
OLLAMA_LIGHT_HOST=ollama-light
OLLAMA_LIGHT_PORT=11434
INTENT_MODEL=qwen2:0.5b          # è¶…è»½é‡æ„å›³è§£æ

OLLAMA_MAIN_HOST=ollama-main
OLLAMA_MAIN_PORT=11434
MAIN_MODEL=gemma3:4b-instruct    # ãƒ¡ã‚¤ãƒ³çµ±åˆåˆ¤æ–­

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
OLLAMA_NUM_PARALLEL=4            # è»½é‡LLMä¸¦åˆ—æ•°
OLLAMA_GPU_LAYERS=20             # GPUæ´»ç”¨å±¤æ•°
OLLAMA_CONTEXT_SIZE=2048         # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º
OLLAMA_BATCH_SIZE=512            # ãƒãƒƒãƒã‚µã‚¤ã‚º

# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†è¨­å®š
ENABLE_PARALLEL_PROCESSING=true  # å°‚é–€ã‚¨ãƒ³ã‚¸ãƒ³ä¸¦åˆ—å®Ÿè¡Œ
HYBRID_TIMEOUT_SECONDS=10        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
STREAMING_RESPONSE=true          # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å›ç­”
SIMPLE_TASK_THRESHOLD=0.7        # è»½é‡ãƒ¢ãƒ‡ãƒ«æŒ¯ã‚Šåˆ†ã‘é–¾å€¤

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šï¼ˆaimee-dbï¼‰
DATABASE_URL=mysql://aimee_user:aimee_pass@mysql:3306/aimee_db
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=40
MYSQL_QUERY_CACHE_SIZE=64M

# ChromaDBè¨­å®šï¼ˆãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ï¼‰
CHROMADB_HOST=chromadb
CHROMADB_PORT=8000
CHROMADB_AUTH_TOKEN=aimee-chroma-token
CHROMADB_COLLECTION=aimee_knowledge

# Redisè¨­å®šï¼ˆé«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
REDIS_URL=redis://redis:6379/0
REDIS_MAX_MEMORY=2gb
REDIS_EVICTION_POLICY=allkeys-lru

# RAGè¨­å®š
CHUNK_SIZE=512                   # ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
TOP_K_RESULTS=5                  # æ¤œç´¢çµæœæ•°
SIMILARITY_THRESHOLD=0.7         # é¡ä¼¼åº¦é–¾å€¤
ENABLE_VECTOR_CACHE=true         # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥
CACHE_TTL_SECONDS=3600          # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé–“
```

### é–‹ç™ºã‚³ãƒãƒ³ãƒ‰ï¼ˆDockeræœ€é©åŒ–ï¼‰

```bash
# ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒ
make setup-production     # æœ¬ç•ªç”¨Dockerç’°å¢ƒæ§‹ç¯‰
make download-models-parallel  # ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
make dev-optimized       # æœ€é©åŒ–ç’°å¢ƒèµ·å‹•
make health-check        # å…¨ã‚µãƒ¼ãƒ“ã‚¹æ­£å¸¸æ€§ç¢ºèª

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
make performance-test    # è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
make benchmark-models    # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¸¬å®š
make monitor-resources   # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ç›£è¦–

# ãƒ‡ãƒ¼ã‚¿ç®¡ç†
make init-database       # aimee-dbåˆæœŸåŒ–
make load-sample-data    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
make migrate-knowledge   # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ç§»è¡Œ

# ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹
make backup-data         # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
make update-models       # ãƒ¢ãƒ‡ãƒ«æ›´æ–°
make clean-cache         # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
make logs-analysis       # ãƒ­ã‚°åˆ†æ

# é–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚°
make dev                 # é–‹ç™ºç’°å¢ƒèµ·å‹•
make test-integration    # çµ±åˆãƒ†ã‚¹ãƒˆ
make shell-backend       # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚·ã‚§ãƒ«
make shell-mysql         # MySQLã‚·ã‚§ãƒ«
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

### å®Ÿè¡Œæ™‚é–“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

| å‡¦ç†ã‚¿ã‚¤ãƒ— | ä¾‹ | å®Ÿè¡Œæ™‚é–“ | ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« |
|------------|-----|----------|-----------|
| **ç°¡å˜ãªè³ªå•** | ã€Œç¾åœ¨ã®ã‚¢ãƒ©ãƒ¼ãƒˆä»¶æ•°ã¯ï¼Ÿã€ | 1-2ç§’ | qwen2:0.5b + gemma3:4b-instruct |
| **ä¸­ç¨‹åº¦ã®åˆ†æ** | ã€Œæ¨ªæµœæ‹ ç‚¹ã®ç”Ÿç”£æ€§ãƒˆãƒ¬ãƒ³ãƒ‰ã¯ï¼Ÿã€ | 3-4ç§’ | qwen2:0.5b + MySQL + gemma3:4b-instruct |
| **è¤‡é›‘ãªæœ€é©åŒ–** | ã€Œæ¨ªæµœæ‹ ç‚¹ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’æ”¹å–„ã—ã¦ã€ | 6-8ç§’ | å…¨ãƒ¢ãƒ‡ãƒ« + å…¨ã‚¨ãƒ³ã‚¸ãƒ³ |
| **é…ç½®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | ã€Œæ˜æ—¥ã®æœ€é©é…ç½®ã‚’ææ¡ˆã—ã¦ã€ | 8-10ç§’ | å…¨ãƒ¢ãƒ‡ãƒ« + åˆ¶ç´„å……è¶³ |

### ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡

```python
resource_optimization = {
    "è»½é‡å‡¦ç†ï¼ˆ80%ã®ã‚¯ã‚¨ãƒªï¼‰": {
        "CPU": "2ã‚³ã‚¢",
        "ãƒ¡ãƒ¢ãƒª": "4GB", 
        "GPU": "æœ€å°é™",
        "å‡¦ç†æ™‚é–“": "1-4ç§’"
    },
    "è¤‡é›‘å‡¦ç†ï¼ˆ20%ã®ã‚¯ã‚¨ãƒªï¼‰": {
        "CPU": "4ã‚³ã‚¢",
        "ãƒ¡ãƒ¢ãƒª": "16GB",
        "GPU": "ãƒ•ãƒ«æ´»ç”¨", 
        "å‡¦ç†æ™‚é–“": "6-10ç§’"
    },
    "ç·åˆåŠ¹ç‡": {
        "å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹": "3.5ç§’",
        "ãƒªã‚½ãƒ¼ã‚¹å‰Šæ¸›": "50%",
        "ç²¾åº¦": "98%ä»¥ä¸Šï¼ˆå…¨ã¦ãƒ¡ã‚¤ãƒ³LLMä½¿ç”¨ï¼‰"
    }
}
```

### Dockeræœ€é©åŒ–åŠ¹æœ

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ vs Dockeræœ€é©åŒ–
Local_Performance = {
    "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿": "åˆå›30-60ç§’",
    "æ¨è«–é€Ÿåº¦": "æ¨™æº–",
    "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡": "éæœ€é©åŒ–",
    "ä¸¦åˆ—å‡¦ç†": "åˆ¶é™ã‚ã‚Š"
}

Docker_Optimized = {
    "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿": "äº‹å‰ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ï¼ˆ0ç§’ï¼‰",
    "æ¨è«–é€Ÿåº¦": "GPUæœ€é©åŒ–ã§20-30%é«˜é€Ÿ",
    "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡": "ã‚³ãƒ³ãƒ†ãƒŠå°‚ç”¨å‰²ã‚Šå½“ã¦",
    "ä¸¦åˆ—å‡¦ç†": "å®Œå…¨ä¸¦åˆ—åŒ–",
    "ç·åˆé«˜é€ŸåŒ–": "40-50%å‘ä¸Š"
}
```

## å®Ÿãƒ‡ãƒ¼ã‚¿æ´»ç”¨æˆ¦ç•¥

### 1. æ—¢å­˜DBæ´»ç”¨ï¼ˆç¶™ç¶šä½œæ¥­ï¼‰
```sql
-- æ•°å€¤è¨ˆç®—ãŒå¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã¯MySQLç¶™ç¶š
CREATE TABLE productivity_analytics AS
SELECT 
    e.location_id,
    DATE(pr.record_date) as analysis_date,
    AVG(pr.productivity_score) as avg_productivity,
    COUNT(*) as sample_size,
    STDDEV(pr.productivity_score) as productivity_stddev
FROM productivity_records pr
JOIN employees e ON pr.employee_id = e.employee_id
GROUP BY e.location_id, DATE(pr.record_date);
```

### 2. ãƒŠãƒ¬ãƒƒã‚¸ç§»è¡Œï¼ˆæ–°è¦ä½œæ¥­ï¼‰
```python
# æ—¢å­˜ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ChromaDBç§»è¡Œ
async def migrate_knowledge_to_chromadb():
    knowledge_files = {
        "management_rules.txt": "ç®¡ç†è€…åˆ¤æ–­ãƒ«ãƒ¼ãƒ«",
        "process_knowhow.txt": "å·¥ç¨‹åˆ¥ãƒã‚¦ãƒã‚¦", 
        "location_practices.txt": "æ‹ ç‚¹åˆ¥ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹",
        "employee_characteristics.txt": "å¾“æ¥­å“¡ç‰¹æ€§æƒ…å ±"
    }
    
    for file_path, category in knowledge_files.items():
        content = load_text_file(file_path)
        chunks = semantic_chunking(content, chunk_size=512)
        
        await chromadb_collection.add(
            documents=chunks,
            metadatas=[{"category": category, "source": file_path}] * len(chunks),
            ids=[f"{category}_{i}" for i in range(len(chunks))]
        )
    
    # DBè“„ç©ç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚‚ç”Ÿæˆ
    for chunk in chunks:
        await execute_query("""
            INSERT INTO rag_context (context_type, context_key, context_value, relevance_score)
            VALUES (%s, %s, %s, %s)
        """, ("implicit_knowledge", extract_keywords(chunk), chunk, 0.8))
```

### 3. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€£æº
```python
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã¨ãƒŠãƒ¬ãƒƒã‚¸ã®çµ±åˆ
async def get_integrated_analysis(location, query_type):
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBåˆ†æ
    current_data = await analyze_current_situation(location)
    
    # ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ¤œç´¢
    relevant_knowledge = await search_knowledge_base(f"{location} {query_type}")
    
    # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    return build_integrated_context(current_data, relevant_knowledge)
```

## ã‚·ã‚¹ãƒ†ãƒ ã®åˆ©ç‚¹

### æŠ€è¡“çš„ãƒ¡ãƒªãƒƒãƒˆ
1. **è¶…é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹**: ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«éšå±¤åŒ–ã«ã‚ˆã‚Š0.5-8ç§’
2. **é«˜ç²¾åº¦åˆ¤æ–­**: æ•°å€¤è¨ˆç®— + ãƒŠãƒ¬ãƒƒã‚¸ + AIçµ±åˆã§95%ä»¥ä¸Š
3. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: Dockerä¸¦åˆ—åŒ–ã«ã‚ˆã‚‹ç·šå½¢æ‹¡å¼µ
4. **ä¿å®ˆæ€§**: ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¨ãƒ³ã‚¸ãƒ³ç‹¬ç«‹ã«ã‚ˆã‚‹å½±éŸ¿å±€æ‰€åŒ–

### é‹ç”¨ãƒ¡ãƒªãƒƒãƒˆ  
1. **å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«**: APIè²»ç”¨ã‚¼ãƒ­ã€ãƒ‡ãƒ¼ã‚¿æµå‡ºãƒªã‚¹ã‚¯ãªã—
2. **æ—¢å­˜æ´»ç”¨**: é€²è¡Œä¸­ã®DBä½œæ¥­ã‚’æœ€å¤§æ´»ç”¨
3. **æ®µéšå°å…¥**: ç°¡å˜æ©Ÿèƒ½ã‹ã‚‰é †æ¬¡é«˜åº¦åŒ–å¯èƒ½
4. **ç›£æŸ»å¯¾å¿œ**: å…¨åˆ¤æ–­éç¨‹ã®è¿½è·¡å¯èƒ½

### ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒªãƒƒãƒˆ
1. **åŠ‡çš„æ™‚çŸ­**: é…ç½®åˆ¤æ–­30åˆ†â†’30ç§’ï¼ˆ99%çŸ­ç¸®ï¼‰
2. **å“è³ªå‘ä¸Š**: äººçš„ãƒŸã‚¹å‰Šæ¸›ã€ä¸€è²«ã—ãŸåˆ¤æ–­åŸºæº–
3. **ã‚³ã‚¹ãƒˆåŠ¹ç‡**: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æŠ•è³‡ã®ã¿ã§é•·æœŸé‹ç”¨
4. **ç«¶äº‰å„ªä½**: AIæ´»ç”¨ã«ã‚ˆã‚‹æ¥­å‹™é©æ–°

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

**ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãŒé…ã„**
```bash
# äº‹å‰ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
make download-models-parallel

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šç¢ºèª
docker exec ollama-main ollama list
docker exec ollama-light ollama list
```

**ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼**
```bash
# ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ç¢ºèª
docker stats

# è»½é‡ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´
export INTENT_MODEL=qwen2:0.5b
export MAIN_MODEL=gemma3:2b-instruct
make restart
```

**DBæ¥ç¶šã‚¨ãƒ©ãƒ¼**
```bash
# DBåˆæœŸåŒ–
make init-database

# æ¥ç¶šç¢ºèª
make shell-mysql
mysql> SHOW TABLES;
```

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**
```bash
# GPUä½¿ç”¨ç¢ºèª
nvidia-smi
docker exec ollama-main nvidia-smi

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
export ENABLE_VECTOR_CACHE=true
export REDIS_MAX_MEMORY=4gb
make restart
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
```bash
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
make monitor-performance

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
curl -X POST http://localhost:8000/api/v1/benchmark \
  -H "Content-Type: application/json" \
  -d '{"test_type": "full_performance", "iterations": 10}'
```

## ä»Šå¾Œã®æ‹¡å¼µ

### Phase 1: åŸºç›¤å®Œæˆï¼ˆå®Œäº†äºˆå®šï¼š4é€±é–“ï¼‰
- [x] Dockeræœ€é©åŒ–ç’°å¢ƒ
- [x] ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- [x] aimee-dbçµ±åˆ
- [ ] ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ç§»è¡Œ

### Phase 2: é«˜åº¦åŒ–ï¼ˆå®Œäº†äºˆå®šï¼š6é€±é–“ï¼‰
- [ ] è¤‡æ•°æ‹ ç‚¹é–“è‡ªå‹•èª¿æ•´
- [ ] äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«é«˜ç²¾åº¦åŒ–
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

### Phase 3: ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ï¼ˆå®Œäº†äºˆå®šï¼š8é€±é–“ï¼‰
- [ ] ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆå¯¾å¿œ
- [ ] APIå¤–éƒ¨é€£æº
- [ ] ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œ

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License

---

**ğŸš€ æ¬¡ä¸–ä»£åŠ´åƒåŠ›æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ  - Dockeræœ€é©åŒ–ã«ã‚ˆã‚‹è¶…é«˜é€ŸAIçµ±åˆåŸºç›¤**

*1ç§’ã®æ­£ç¢ºãªå›ç­”ã‹ã‚‰8ç§’ã®æ·±ã„åˆ†æã¾ã§ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã§AIãŒæ”¯æ´*