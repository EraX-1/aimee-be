#!/usr/bin/env python3
"""
Llamaã®åŸºæœ¬å‹•ä½œãƒ†ã‚¹ãƒˆ
M3 Macã§ã®å‹•ä½œç¢ºèªç”¨
"""

import time
from llama_cpp import Llama

def test_basic_generation():
    """åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    print("ğŸ¤– Llamaãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    # M3 Macç”¨ã®è¨­å®š
    llm = Llama(
        model_path="./models/llama-2-7b-chat.Q4_K_M.gguf",
        n_ctx=2048,        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·
        n_threads=8,       # M3ã®ã‚³ã‚¢æ•°ã«åˆã‚ã›ã¦
        n_gpu_layers=1,    # Metal GPUä½¿ç”¨
        use_mlock=False,   # ãƒ¡ãƒ¢ãƒªã‚¹ãƒ¯ãƒƒãƒ—è¨±å¯
    )
    
    print("âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\n")
    
    # ãƒ†ã‚¹ãƒˆ1: ã‚·ãƒ³ãƒ—ãƒ«ãªè³ªå•
    print("ğŸ“ ãƒ†ã‚¹ãƒˆ1: ã‚·ãƒ³ãƒ—ãƒ«ãªè³ªå•")
    start_time = time.time()
    
    prompt = "What is the capital of Japan?"
    response = llm(prompt, max_tokens=50)
    
    print(f"è³ªå•: {prompt}")
    print(f"å›ç­”: {response['choices'][0]['text']}")
    print(f"ç”Ÿæˆæ™‚é–“: {time.time() - start_time:.2f}ç§’\n")
    
    # ãƒ†ã‚¹ãƒˆ2: æ—¥æœ¬èªå¯¾å¿œç¢ºèª
    print("ğŸ“ ãƒ†ã‚¹ãƒˆ2: æ—¥æœ¬èªã§ã®è³ªå•")
    start_time = time.time()
    
    prompt = "äººå·¥çŸ¥èƒ½ã®åˆ©ç‚¹ã‚’3ã¤æ•™ãˆã¦ãã ã•ã„ã€‚"
    response = llm(prompt, max_tokens=200)
    
    print(f"è³ªå•: {prompt}")
    print(f"å›ç­”: {response['choices'][0]['text']}")
    print(f"ç”Ÿæˆæ™‚é–“: {time.time() - start_time:.2f}ç§’\n")

def test_chat_format():
    """ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®ãƒ†ã‚¹ãƒˆ")
    
    llm = Llama(
        model_path="./models/llama-2-7b-chat.Q4_K_M.gguf",
        n_ctx=2048,
        n_threads=8,
        n_gpu_layers=1,
    )
    
    # Llama-2ã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    system_prompt = "You are a helpful assistant for workforce management."
    user_message = "æ¨ªæµœæ‹ ç‚¹ã®ç”Ÿç”£æ€§ãŒä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚ã©ã†å¯¾å¿œã™ã¹ãã§ã™ã‹ï¼Ÿ"
    
    prompt = f"""<s>[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_message} [/INST]"""
    
    start_time = time.time()
    response = llm(prompt, max_tokens=300)
    
    print(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {system_prompt}")
    print(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_message}")
    print(f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {response['choices'][0]['text']}")
    print(f"ç”Ÿæˆæ™‚é–“: {time.time() - start_time:.2f}ç§’\n")

def check_memory_usage():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª"""
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    
    print(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³:")
    print(f"  - RSS: {memory_info.rss / 1024 / 1024 / 1024:.2f} GB")
    print(f"  - VMS: {memory_info.vms / 1024 / 1024 / 1024:.2f} GB")

if __name__ == "__main__":
    print("ğŸš€ Llamaå‹•ä½œãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...\n")
    
    try:
        test_basic_generation()
        test_chat_format()
        check_memory_usage()
        
        print("\nâœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")