#!/usr/bin/env python3
"""
ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°å®Ÿè£…
æ„å‘³çš„ã«é–¢é€£ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ã¾ã¨ã‚ã¦åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã€RAGã®ç²¾åº¦ã‚’å‘ä¸Š
"""

import numpy as np
from typing import List, Tuple
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.tokenize import sent_tokenize
import re

class SemanticChunker:
    def __init__(self, model_name="intfloat/multilingual-e5-base", threshold=0.75):
        """
        ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚«ãƒ¼ã®åˆæœŸåŒ–
        
        Args:
            model_name: æ–‡åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å
            threshold: é¡ä¼¼åº¦ã®é–¾å€¤ï¼ˆ0.0-1.0ï¼‰
        """
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold
        
        # æ—¥æœ¬èªã®æ–‡åˆ†å‰²ç”¨
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
    
    def _split_sentences(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰"""
        # æ—¥æœ¬èªã®æ–‡æœ«è¨˜å·ã§åˆ†å‰²
        japanese_pattern = r'[ã€‚ï¼ï¼Ÿ\n]+'
        english_pattern = r'[.!?\n]+'
        
        # æ—¥æœ¬èªã¨è‹±èªã®ä¸¡æ–¹ã«å¯¾å¿œ
        sentences = re.split(f'{japanese_pattern}|{english_pattern}', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def _calculate_similarity_matrix(self, sentences: List[str]) -> np.ndarray:
        """æ–‡é–“ã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—"""
        embeddings = self.model.encode(sentences)
        similarity_matrix = cosine_similarity(embeddings)
        return similarity_matrix
    
    def _group_similar_sentences(self, sentences: List[str], similarity_matrix: np.ndarray) -> List[List[str]]:
        """é¡ä¼¼åº¦ã«åŸºã¥ã„ã¦æ–‡ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        n = len(sentences)
        groups = []
        used = set()
        
        for i in range(n):
            if i in used:
                continue
            
            # æ–°ã—ã„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹
            group = [sentences[i]]
            used.add(i)
            
            # å¾Œç¶šã®æ–‡ã§é¡ä¼¼åº¦ãŒé–¾å€¤ã‚’è¶…ãˆã‚‹ã‚‚ã®ã‚’è¿½åŠ 
            for j in range(i + 1, min(i + 10, n)):  # æœ€å¤§10æ–‡å…ˆã¾ã§è¦‹ã‚‹
                if j not in used and similarity_matrix[i][j] >= self.threshold:
                    group.append(sentences[j])
                    used.add(j)
                elif similarity_matrix[i][j] < self.threshold * 0.7:  # å¤§ããç•°ãªã‚‹å ´åˆã¯æ‰“ã¡åˆ‡ã‚Š
                    break
            
            groups.append(group)
        
        return groups
    
    def chunk_text(self, text: str, max_chunk_size: int = 500, min_chunk_size: int = 100) -> List[str]:
        """
        ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã‚’å®Ÿè¡Œ
        
        Args:
            text: åˆ†å‰²ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            max_chunk_size: ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§æ–‡å­—æ•°
            min_chunk_size: ãƒãƒ£ãƒ³ã‚¯ã®æœ€å°æ–‡å­—æ•°
        
        Returns:
            ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        # æ–‡ã«åˆ†å‰²
        sentences = self._split_sentences(text)
        if not sentences:
            return []
        
        # é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—
        similarity_matrix = self._calculate_similarity_matrix(sentences)
        
        # æ–‡ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        groups = self._group_similar_sentences(sentences, similarity_matrix)
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ
        chunks = []
        current_chunk = []
        current_size = 0
        
        for group in groups:
            group_text = ' '.join(group)
            group_size = len(group_text)
            
            if current_size + group_size > max_chunk_size and current_chunk:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text) >= min_chunk_size:
                    chunks.append(chunk_text)
                current_chunk = [group_text]
                current_size = group_size
            else:
                current_chunk.append(group_text)
                current_size += group_size
        
        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text) >= min_chunk_size:
                chunks.append(chunk_text)
        
        return chunks

class HybridChunker:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã¨æ§‹é€ çš„ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰"""
    
    def __init__(self, semantic_chunker: SemanticChunker):
        self.semantic_chunker = semantic_chunker
    
    def chunk_with_overlap(self, text: str, overlap_size: int = 50) -> List[str]:
        """ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ä»˜ããƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿æŒï¼‰"""
        base_chunks = self.semantic_chunker.chunk_text(text)
        
        if len(base_chunks) <= 1:
            return base_chunks
        
        # éš£æ¥ãƒãƒ£ãƒ³ã‚¯é–“ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è¿½åŠ 
        enhanced_chunks = []
        for i, chunk in enumerate(base_chunks):
            if i > 0:
                # å‰ã®ãƒãƒ£ãƒ³ã‚¯ã®æœ«å°¾ã‚’è¿½åŠ 
                prev_words = base_chunks[i-1].split()[-overlap_size//10:]
                chunk = ' '.join(prev_words) + ' ' + chunk
            
            if i < len(base_chunks) - 1:
                # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®å…ˆé ­ã‚’è¿½åŠ 
                next_words = base_chunks[i+1].split()[:overlap_size//10]
                chunk = chunk + ' ' + ' '.join(next_words)
            
            enhanced_chunks.append(chunk.strip())
        
        return enhanced_chunks
    
    def chunk_with_metadata(self, text: str, metadata: dict = None) -> List[Tuple[str, dict]]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ããƒãƒ£ãƒ³ã‚­ãƒ³ã‚°"""
        chunks = self.semantic_chunker.chunk_text(text)
        
        result = []
        for i, chunk in enumerate(chunks):
            chunk_metadata = metadata.copy() if metadata else {}
            chunk_metadata.update({
                'chunk_index': i,
                'total_chunks': len(chunks),
                'chunk_method': 'semantic'
            })
            result.append((chunk, chunk_metadata))
        
        return result

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    sample_text = """
    æ¨ªæµœæ‹ ç‚¹ã«ã¯ç¾åœ¨80åã®å¾“æ¥­å“¡ãŒåœ¨ç±ã—ã¦ã„ã¾ã™ã€‚ä¸»ã«ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ¥­å‹™ã‚’æ‹…å½“ã—ã¦ãŠã‚Šã€24æ™‚é–“ä½“åˆ¶ã§ç¨¼åƒã—ã¦ã„ã¾ã™ã€‚
    æœ€è¿‘ã®ç”Ÿç”£æ€§ã¯ç›®æ¨™å€¤ã®85%ç¨‹åº¦ã§æ¨ç§»ã—ã¦ã„ã¾ã™ã€‚æœˆæ›œæ—¥ã®æœã¯ç‰¹ã«ç”Ÿç”£æ€§ãŒä½ä¸‹ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚
    
    ç››å²¡æ‹ ç‚¹ã¯30åä½“åˆ¶ã§é‹å–¶ã•ã‚Œã¦ã„ã¾ã™ã€‚è£œæ­£å‡¦ç†æ¥­å‹™ã«ç‰¹åŒ–ã—ã¦ãŠã‚Šã€ç†Ÿç·´è€…ãŒå¤šãåœ¨ç±ã—ã¦ã„ã¾ã™ã€‚
    ç”Ÿç”£æ€§ã¯å®‰å®šã—ã¦ç›®æ¨™å€¤ã®110%ã‚’é”æˆã—ã¦ãŠã‚Šã€ä»–æ‹ ç‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚±ãƒ¼ã‚¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚
    
    äººå“¡é…ç½®ã®æœ€é©åŒ–ã«ã¯ã€å„å¾“æ¥­å“¡ã®ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ«ã¨ä½œæ¥­åŠ¹ç‡ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    æ–°äººæ•™è‚²ã§ã¯ã€ç†Ÿç·´è€…ã¨ã®ãƒšã‚¢ä½œæ¥­ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ãŒåŠ¹æœçš„ã§ã™ã€‚
    """
    
    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚«ãƒ¼ã®åˆæœŸåŒ–
    chunker = SemanticChunker(threshold=0.7)
    
    print("ğŸ” ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°çµæœ:")
    chunks = chunker.chunk_text(sample_text)
    for i, chunk in enumerate(chunks, 1):
        print(f"\nãƒãƒ£ãƒ³ã‚¯ {i}:")
        print(chunk)
        print("-" * 50)
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒãƒ£ãƒ³ã‚«ãƒ¼ã®ä½¿ç”¨
    hybrid = HybridChunker(chunker)
    
    print("\n\nğŸ”„ ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ä»˜ããƒãƒ£ãƒ³ã‚­ãƒ³ã‚°:")
    overlap_chunks = hybrid.chunk_with_overlap(sample_text, overlap_size=30)
    for i, chunk in enumerate(overlap_chunks, 1):
        print(f"\nãƒãƒ£ãƒ³ã‚¯ {i}:")
        print(chunk)
        print("-" * 50)