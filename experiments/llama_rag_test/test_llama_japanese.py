#!/usr/bin/env python3
"""
Llamaæ—¥æœ¬èªå°‚ç”¨ãƒ†ã‚¹ãƒˆ
å¿…ãšæ—¥æœ¬èªã§å›ç­”ã™ã‚‹ãŸã‚ã®è¨­å®š
"""

import time
from llama_cpp import Llama

def create_japanese_llm():
    """æ—¥æœ¬èªç‰¹åŒ–ã®Llamaãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    print("ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œLlamaãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    llm = Llama(
        model_path="./models/llama-2-7b-chat.Q4_K_M.gguf",
        n_ctx=2048,
        n_threads=8,
        n_gpu_layers=1,
    )
    
    print("âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\n")
    return llm

def japanese_prompt(user_message, system_message=None):
    """æ—¥æœ¬èªå›ç­”ã‚’å¼·åˆ¶ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
    
    if system_message is None:
        system_message = """ã‚ãªãŸã¯æ—¥æœ¬èªã§å›ç­”ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®é‡è¦ãªãƒ«ãƒ¼ãƒ«ã‚’å¿…ãšå®ˆã£ã¦ãã ã•ã„ï¼š
1. å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„
2. è‹±èªã‚„ä»–ã®è¨€èªã¯ä½¿ã‚ãªã„ã§ãã ã•ã„
3. å°‚é–€ç”¨èªã‚‚å¯èƒ½ãªé™ã‚Šæ—¥æœ¬èªã«ã—ã¦ãã ã•ã„
4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‹±èªã§è³ªå•ã—ã¦ã‚‚ã€æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„"""
    
    prompt = f"""<s>[INST] <<SYS>>
{system_message}
<</SYS>>

{user_message} [/INST]

å›ç­”ï¼ˆæ—¥æœ¬èªã§ï¼‰ï¼š"""
    
    return prompt

def test_japanese_responses():
    """æ—¥æœ¬èªå›ç­”ã®ãƒ†ã‚¹ãƒˆ"""
    llm = create_japanese_llm()
    
    test_cases = [
        # è‹±èªã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã‚‹
        {
            "question": "What is AI?",
            "expected": "æ—¥æœ¬èªã§ã®å›ç­”"
        },
        # æ—¥æœ¬èªã®è³ªå•
        {
            "question": "äººå·¥çŸ¥èƒ½ã®åˆ©ç‚¹ã‚’3ã¤æ•™ãˆã¦ãã ã•ã„ã€‚",
            "expected": "æ—¥æœ¬èªã§ã®å›ç­”"
        },
        # æ¥­å‹™é–¢é€£ã®è³ªå•
        {
            "question": "How can we improve productivity in Yokohama office?",
            "expected": "æ—¥æœ¬èªã§ã®å›ç­”"
        },
        # æ··åœ¨ã—ãŸè³ªå•
        {
            "question": "Llama modelã®ä½¿ã„æ–¹ã‚’èª¬æ˜ã—ã¦",
            "expected": "æ—¥æœ¬èªã§ã®å›ç­”"
        }
    ]
    
    for i, test in enumerate(test_cases, 1):
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆ{i}: {test['question']}")
        
        # æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ©ãƒƒãƒ—
        prompt = japanese_prompt(test['question'])
        
        start_time = time.time()
        response = llm(prompt, max_tokens=200, temperature=0.7)
        answer = response['choices'][0]['text'].strip()
        
        print(f"ğŸ’¬ å›ç­”: {answer}")
        print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {time.time() - start_time:.2f}ç§’")
        print("-" * 50)

def test_business_japanese():
    """ãƒ“ã‚¸ãƒã‚¹ç”¨é€”ã®æ—¥æœ¬èªãƒ†ã‚¹ãƒˆ"""
    llm = create_japanese_llm()
    
    print("\nğŸ¢ ãƒ“ã‚¸ãƒã‚¹ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ")
    
    # ãƒ“ã‚¸ãƒã‚¹ç‰¹åŒ–ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    business_system = """ã‚ãªãŸã¯æ—¥æœ¬ä¼æ¥­ã®åŠ´åƒåŠ›ç®¡ç†ã‚’æ”¯æ´ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
å¿…ãšä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
1. å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã†
2. æ•¬èªã‚’é©åˆ‡ã«ä½¿ç”¨ã™ã‚‹
3. ãƒ“ã‚¸ãƒã‚¹ç”¨èªã¯æ—¥æœ¬èªã§è¡¨ç¾ã™ã‚‹
4. æ•°å€¤ã¯å…¨è§’ã§ã¯ãªãåŠè§’ã‚’ä½¿ç”¨ã™ã‚‹
5. ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã‚’å¿ƒãŒã‘ã‚‹"""
    
    questions = [
        "What is the current productivity rate?",
        "æ¨ªæµœæ‹ ç‚¹ã®äººå“¡é…ç½®ã‚’æœ€é©åŒ–ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã—ã¦ãã ã•ã„",
        "Monday morningã®ç”Ÿç”£æ€§ãŒä½ã„ç†ç”±ã¨å¯¾ç­–ã‚’æ•™ãˆã¦",
    ]
    
    for question in questions:
        print(f"\nâ“ è³ªå•: {question}")
        
        prompt = japanese_prompt(question, business_system)
        response = llm(prompt, max_tokens=300, temperature=0.5)
        answer = response['choices'][0]['text'].strip()
        
        print(f"ğŸ“Š å›ç­”: {answer}")

def test_rag_japanese():
    """RAGä½¿ç”¨æ™‚ã®æ—¥æœ¬èªå›ç­”ãƒ†ã‚¹ãƒˆ"""
    llm = create_japanese_llm()
    
    print("\nğŸ“š RAG + æ—¥æœ¬èªå›ç­”ãƒ†ã‚¹ãƒˆ")
    
    # RAGç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè‹±èªæ··åœ¨ï¼‰
    context = """
    æ¨ªæµœæ‹ ç‚¹: 80ååœ¨ç±, productivity 85%
    ç››å²¡æ‹ ç‚¹: 30ååœ¨ç±, productivity 110%
    Main issues: High absence rate on Monday morning
    Recommendation: Skill matching and workload balancing needed
    """
    
    question = "å„æ‹ ç‚¹ã®çŠ¶æ³ã¨æ”¹å–„ç­–ã‚’æ•™ãˆã¦ãã ã•ã„"
    
    # RAGå¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    rag_prompt = f"""<s>[INST] <<SYS>>
ã‚ãªãŸã¯æ—¥æœ¬èªã§å›ç­”ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚è€ƒã«ã€å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
è‹±èªã®æƒ…å ±ã‚‚æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
<</SYS>>

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {question}

æ—¥æœ¬èªã§å›ç­”: [/INST]"""
    
    print(f"ğŸ“„ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè‹±èªæ··åœ¨ï¼‰:\n{context}")
    print(f"\nâ“ è³ªå•: {question}")
    
    response = llm(rag_prompt, max_tokens=400, temperature=0.3)
    answer = response['choices'][0]['text'].strip()
    
    print(f"\nğŸ’¡ å›ç­”: {answer}")

def create_japanese_enforcer_wrapper(llm):
    """æ—¥æœ¬èªã‚’å¼·åˆ¶ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°"""
    def japanese_only_llm(user_input, **kwargs):
        # è‡ªå‹•çš„ã«æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ©ãƒƒãƒ—
        prompt = japanese_prompt(user_input)
        return llm(prompt, **kwargs)
    
    return japanese_only_llm

if __name__ == "__main__":
    print("ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå°‚ç”¨Llamaãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...\n")
    
    try:
        test_japanese_responses()
        test_business_japanese()
        test_rag_japanese()
        
        print("\nâœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("\nğŸ’¡ ãƒ’ãƒ³ãƒˆ:")
        print("- ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã€Œå¿…ãšæ—¥æœ¬èªã§ã€ã¨æ˜è¨˜")
        print("- è‹±èªã®è³ªå•ã«ã‚‚æ—¥æœ¬èªã§å›ç­”ã™ã‚‹ã‚ˆã†æŒ‡ç¤º")
        print("- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¾Œã«ã€Œæ—¥æœ¬èªã§å›ç­”ï¼šã€ã‚’è¿½åŠ ")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")