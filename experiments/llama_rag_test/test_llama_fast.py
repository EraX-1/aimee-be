#!/usr/bin/env python3
"""
Llamaé«˜é€Ÿå‹•ä½œãƒ†ã‚¹ãƒˆç‰ˆ
å›ç­”æ™‚é–“ã‚’æœ€é©åŒ–ã—ãŸè¨­å®š
"""

import time
from llama_cpp import Llama
import hashlib
from functools import lru_cache

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆèµ·å‹•æ™‚é–“çŸ­ç¸®ï¼‰
print("ğŸš€ é«˜é€Ÿç‰ˆLlamaãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
llm = Llama(
    model_path="./models/llama-2-7b-chat.Q4_K_M.gguf",  # Q3_K_SãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’ä½¿ç”¨
    n_ctx=1024,         # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’çŸ­ç¸®
    n_batch=256,        # ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
    n_threads=8,        # M3ã®ã‚³ã‚¢æ•°ã«æœ€é©åŒ–
    n_gpu_layers=32,    # Metal GPUæœ€å¤§æ´»ç”¨
    use_mlock=False,
    use_mmap=True,      # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã§é«˜é€ŸåŒ–
    verbose=False,      # ãƒ­ã‚°å‡ºåŠ›ã‚’æŠ‘åˆ¶
)

@lru_cache(maxsize=100)
def cached_response(prompt_hash, max_tokens=100):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãå¿œç­”ç”Ÿæˆ"""
    return llm(prompt_hash, max_tokens=max_tokens)

def test_speed_optimization():
    """é€Ÿåº¦æœ€é©åŒ–ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nâš¡ é€Ÿåº¦æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ")
    
    # ãƒ†ã‚¹ãƒˆ1: çŸ­ã„å¿œç­”
    print("\nğŸ“ ãƒ†ã‚¹ãƒˆ1: çŸ­ã„å¿œç­”ï¼ˆmax_tokens=50ï¼‰")
    prompt = "What is 2+2? Answer in one sentence."
    
    start_time = time.time()
    response = llm(prompt, max_tokens=50, temperature=0.1)
    end_time = time.time()
    
    print(f"è³ªå•: {prompt}")
    print(f"å›ç­”: {response['choices'][0]['text'].strip()}")
    print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
    
    # ãƒ†ã‚¹ãƒˆ2: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”
    print("\nğŸ“ ãƒ†ã‚¹ãƒˆ2: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”")
    prompt = "List 3 benefits of exercise:"
    
    print(f"è³ªå•: {prompt}")
    print("å›ç­”: ", end='')
    
    start_time = time.time()
    for token in llm(prompt, max_tokens=100, stream=True, temperature=0.5):
        print(token['choices'][0]['text'], end='', flush=True)
    end_time = time.time()
    
    print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")

def test_batch_processing():
    """ãƒãƒƒãƒå‡¦ç†ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“¦ ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ")
    
    questions = [
        "æ¨ªæµœæ‹ ç‚¹ã®çŠ¶æ³ã¯ï¼Ÿ",
        "ç”Ÿç”£æ€§ã‚’ä¸Šã’ã‚‹æ–¹æ³•ã¯ï¼Ÿ",
        "æœˆæ›œã®æ³¨æ„ç‚¹ã¯ï¼Ÿ"
    ]
    
    # Llama-2ã®ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼
    batch_prompt = f"""<s>[INST] <<SYS>>
ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚å„è³ªå•ã«1-2æ–‡ã§ç­”ãˆã¦ãã ã•ã„ã€‚
<</SYS>>

è³ªå•1: {questions[0]}
è³ªå•2: {questions[1]}
è³ªå•3: {questions[2]}

å›ç­”: [/INST]"""
    
    start_time = time.time()
    response = llm(batch_prompt, max_tokens=200, temperature=0.3)
    end_time = time.time()
    
    print("ãƒãƒƒãƒå›ç­”:")
    print(response['choices'][0]['text'])
    print(f"\nâ±ï¸ 3ã¤ã®è³ªå•ã®ç·ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
    print(f"ğŸ“Š 1è³ªå•ã‚ãŸã‚Š: {(end_time - start_time) / 3:.2f}ç§’")

def test_cache_performance():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ€§èƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
    
    prompt = "What are the benefits of using AI in workforce management?"
    
    # åˆå›å®Ÿè¡Œ
    start_time = time.time()
    response1 = llm(prompt, max_tokens=100)
    first_time = time.time() - start_time
    
    # 2å›ç›®å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
    start_time = time.time()
    response2 = llm(prompt, max_tokens=100)
    second_time = time.time() - start_time
    
    print(f"â±ï¸ åˆå›å®Ÿè¡Œ: {first_time:.2f}ç§’")
    print(f"â±ï¸ 2å›ç›®å®Ÿè¡Œ: {second_time:.2f}ç§’")
    print(f"ğŸ“ˆ é€Ÿåº¦å‘ä¸Š: {((first_time - second_time) / first_time * 100):.1f}%")

def print_optimization_tips():
    """æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ"""
    print("\nğŸ’¡ æ›´ãªã‚‹é«˜é€ŸåŒ–ã®ãƒ’ãƒ³ãƒˆ:")
    print("1. ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆQ3_K_Sï¼‰ã‚’ä½¿ç”¨")
    print("2. n_ctxï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼‰ã‚’å¿…è¦æœ€å°é™ã«")
    print("3. max_tokensã‚’é©åˆ‡ã«åˆ¶é™")
    print("4. temperatureã‚’ä½ãè¨­å®šï¼ˆ0.1-0.3ï¼‰")
    print("5. Metal GPUãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æœ€å¤§åŒ–")
    print("6. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡æ½”ã«")
    print("7. é »ç¹ãªè³ªå•ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥")

if __name__ == "__main__":
    print("ğŸš€ Llamaé«˜é€Ÿå‹•ä½œãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...\n")
    
    try:
        test_speed_optimization()
        test_batch_processing()
        test_cache_performance()
        print_optimization_tips()
        
        print("\nâœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")